{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRCNN_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artiumb/IRIS_SR_CNN/blob/master/SRCNN_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSoeCL3Mq8vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some references:\n",
        "# https://github.com/Mirwaisse/SRCNN \n",
        "# https://github.com/basher666/pytorch_srcnn\n",
        "# https://github.com/MarkPrecursor/SRCNN-keras <-based on this\n",
        "# https://github.com/thuyngch/Iris-Recognition-PyTorch\n",
        "# http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html : original paper site\n",
        "#paper:\n",
        "# http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepresolution.pdf\n",
        "#updated paper 2015:\n",
        "# https://arxiv.org/pdf/1501.00092.pdf\n",
        "#resnet for SR:\n",
        "# https://github.com/yulunzhang/RDN#results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzD1988B5VgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChxAklkSXrzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy 3 datasets (high and low images ) from drive to local Env in colab\n",
        "\n",
        "if (False):\n",
        "    !cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/IrisResizedOriginal/Dataset/Original-240x320/Casia-v4-240x320/.' '/content/Dataset_casia_h'\n",
        "    print('casia done')\n",
        "    !cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/IrisResizedOriginal/Dataset/Original-240x320/berc-240x320/.' '/content/Dataset_berc_h'\n",
        "    print('berc done')\n",
        "    !cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/IrisResizedOriginal/Dataset/Original-240x320/ndcld-240x320/.' '/content/Dataset_ndcld_h'\n",
        "    # Drive/Toar2/Thesis/Iris_recog/IrisResizedOriginal.zip (Unzipped Files)/IrisResizedOriginal/Dataset/Original-240x320/ndcld-240x320\n",
        "    print('ndcld done')\n",
        "    !cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/IrisResizedOriginal/Dataset/Resize-60x80/Casia-v4-60x80/.' '/content/Dataset_casia_l'\n",
        "    print('casia done')\n",
        "    !cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/IrisResizedOriginal/Dataset/Resize-60x80/berc-60x80/.' '/content/Dataset_berc_l'\n",
        "    print('berc done')\n",
        "    !cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/IrisResizedOriginal/Dataset/Resize-60x80/ndcld-60x80/.' '/content/Dataset_ndcld_l'\n",
        "    print('ndcld done')\n",
        "    \n",
        " # unite datasets:\n",
        "    !cp -ar '/content/Dataset_casia_h/.' '/content/CombinedDataset_h'\n",
        "    !cp -ar '/content/Dataset_berc_h/.' '/content/CombinedDataset_h'\n",
        "    !cp -ar '/content/Dataset_ndcld_h/.' '/content/CombinedDataset_h'\n",
        "# unite datasets:\n",
        "    !cp -ar '/content/Dataset_casia_l/.' '/content/CombinedDataset_l'\n",
        "    !cp -ar '/content/Dataset_berc_l/.' '/content/CombinedDataset_l'\n",
        "    !cp -ar '/content/Dataset_ndcld_l/.' '/content/CombinedDataset_l'\n",
        "    print('combine done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLuVBpTCUS_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split dataset directory to train and test subdirs (random)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from shutil import copyfile\n",
        "\n",
        "def img_train_test_split_per_person(img_source_dir_l,img_source_dir_h, train_size,dataset_name):\n",
        "\n",
        "    # High res   \n",
        "    if not (isinstance(img_source_dir_h, str)):\n",
        "        raise AttributeError('img_source_dir must be a string')\n",
        "    if not os.path.exists(img_source_dir_h):\n",
        "        raise OSError('img_source_dir does not exist')    \n",
        "    if not (isinstance(train_size, float)):\n",
        "        raise AttributeError('train_size must be a float')        \n",
        "    if not os.path.exists(img_source_dir_h + '/data'):\n",
        "        os.makedirs(img_source_dir_h + '/data')\n",
        "    else:\n",
        "        if not os.path.exists(img_source_dir_h + '/data/train'):\n",
        "            os.makedirs(img_source_dir_h + '/data/train')\n",
        "        if not os.path.exists(img_source_dir_h + '/data/validation'):\n",
        "            os.makedirs(img_source_dir_h + '/data/validation')\n",
        "            \n",
        "        train_subdir_h = img_source_dir_h + '/data/train'\n",
        "        validation_subdir_h = img_source_dir_h + '/data/validation'\n",
        "#low res\n",
        "    if not (isinstance(img_source_dir_l, str)):\n",
        "        raise AttributeError('img_source_dir must be a string')\n",
        "    if not os.path.exists(img_source_dir_l):\n",
        "        raise OSError('img_source_dir does not exist')    \n",
        "    if not (isinstance(train_size, float)):\n",
        "        raise AttributeError('train_size must be a float')        \n",
        "    if not os.path.exists(img_source_dir_l + '/data'):\n",
        "        os.makedirs(img_source_dir_l + '/data')\n",
        "    else:\n",
        "        if not os.path.exists(img_source_dir_l + '/data/train'):\n",
        "            os.makedirs(img_source_dir_l + '/data/train')\n",
        "        if not os.path.exists(img_source_dir_l + '/data/validation'):\n",
        "            os.makedirs(img_source_dir_l + '/data/validation')\n",
        "            \n",
        "        train_subdir_l = img_source_dir_l + '/data/train'\n",
        "        validation_subdir_l = img_source_dir_l + '/data/validation'\n",
        "\n",
        "        # Create subdirectories in train and validation folders\n",
        "        if not os.path.exists(train_subdir_h):\n",
        "            os.makedirs(train_subdir_h)\n",
        "        if not os.path.exists(validation_subdir_h):\n",
        "            os.makedirs(validation_subdir_h)\n",
        "        if not os.path.exists(train_subdir_l):\n",
        "            os.makedirs(train_subdir_l)\n",
        "        if not os.path.exists(validation_subdir_l):\n",
        "            os.makedirs(validation_subdir_l)\n",
        "\n",
        "        train_counter = 0\n",
        "        validation_counter = 0\n",
        "        train_name_counter = 0\n",
        "        validation_name_counter = 0\n",
        "        subdir_fullpath = img_source_dir_h\n",
        "        name_list_train = []\n",
        "        name_list_val = []\n",
        "        print(dataset_name)\n",
        "        # Randomly assign an image to train or validation folder\n",
        "        for filename in os.listdir(subdir_fullpath):\n",
        "            if filename.endswith(\".jpg\"): \n",
        "                fileparts = filename.split('-')\n",
        "                #each dataset marks person if in different way\n",
        "                if (dataset_name == \"casia\"):\n",
        "                    person_id = re.split ('L|R',str(fileparts[3]))[0]\n",
        "                if (dataset_name == \"berc\"):\n",
        "                     person_id = re.split ('_l_',str(fileparts[1] +  fileparts[2]))[0]\n",
        "                if (dataset_name == \"ndcld\"):\n",
        "                    person_id = re.split( 'd', str(fileparts[2]).strip('.jpg'))[0]\n",
        "                # parse ids\n",
        "                print(person_id)\n",
        "                if random.uniform(0, 1) <= train_size:\n",
        "                    if person_id not in name_list_train:\n",
        "                        if person_id not in name_list_val:\n",
        "                            name_list_train.append(person_id)\n",
        "                            train_name_counter += 1\n",
        "                else:\n",
        "                    if person_id not in name_list_train:\n",
        "                        if person_id not in name_list_val:\n",
        "                            name_list_val.append(person_id)\n",
        "                            validation_name_counter += 1 \n",
        "        print('done names' )\n",
        "        print(' Total: val names' + str(validation_name_counter) + ' train names' +str(train_name_counter) )  \n",
        "         \n",
        "        for filename in os.listdir(subdir_fullpath):\n",
        "            if filename.endswith(\".jpg\"): \n",
        "                fileparts = filename.split('-')\n",
        "                #each dataset marks person if in different way\n",
        "                if (dataset_name == \"casia\"):\n",
        "                    person_id = re.split ('L|R',str(fileparts[3]))[0]\n",
        "                if (dataset_name == \"berc\"):\n",
        "                     person_id = re.split ('_l_',str(fileparts[1] +  fileparts[2]))[0]\n",
        "                if (dataset_name == \"ndcld\"):\n",
        "                    person_id = re.split( 'd', str(fileparts[2]).strip('.jpg'))[0]\n",
        "\n",
        "                if person_id in name_list_train:\n",
        "                    copyfile(os.path.join(img_source_dir_h, filename), os.path.join(train_subdir_h, filename))\n",
        "                    copyfile(os.path.join(img_source_dir_l, filename), os.path.join(train_subdir_l, filename))\n",
        "                    train_counter += 1\n",
        "                if person_id  in name_list_val:\n",
        "                    copyfile(os.path.join(img_source_dir_h, filename), os.path.join(validation_subdir_h,filename))\n",
        "                    copyfile(os.path.join(img_source_dir_l, filename), os.path.join(validation_subdir_l,filename))\n",
        "                    validation_counter += 1     \n",
        "                      \n",
        "              \n",
        "        print('Copied ' + str(train_counter) + ' images to data/train/' )\n",
        "        print('Copied ' + str(validation_counter) + ' images to data/validation/' )\n",
        "        \n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNfyzCQaAn2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split datasets to train and test with 0.7 factor\n",
        "if (False):\n",
        "    img_train_test_split_per_person('/content/Dataset_berc_l/','/content/Dataset_berc_h/',0.7,\"berc\")\n",
        "    img_train_test_split_per_person('/content/Dataset_casia_l/','/content/Dataset_casia_h/',0.7,\"casia\")\n",
        "    img_train_test_split_per_person('/content/Dataset_ndcld_l/','/content/Dataset_ndcld_h/',0.7,\"ndcld\")\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMiXQ6VeX6Qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf '/content/Dataset_berc_l/data'\n",
        "!rm -rf '/content/Dataset_berc_h/data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU8XS-N1-mjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create test and validation\n",
        "from shutil import copytree\n",
        "if (False):\n",
        "\n",
        "# save backup of split datasets to to drive:\n",
        "    copytree('/content/Dataset_ndcld_h/data', os.path.join('/content/drive/My Drive/Toar2/Thesis/Iris_recog/' 'Dataset_ndcld_split_h'))\n",
        "    copytree('/content/Dataset_casia_h/data', os.path.join('/content/drive/My Drive/Toar2/Thesis/Iris_recog/' 'Dataset_casia_split_h'))\n",
        "    copytree('/content/Dataset_berc_h/data', os.path.join('/content/drive/My Drive/Toar2/Thesis/Iris_recog/' 'Dataset_berc_split_h'))\n",
        "    copytree('/content/CombinedDataset_h/data', os.path.join('/content/drive/My Drive/Toar2/Thesis/Iris_recog/' 'CombinedDataset_split_h'))\n",
        "\n",
        "    copytree('/content/Dataset_ndcld_l/data', os.path.join('/content/drive/My Drive/Toar2/Thesis/Iris_recog/' 'Dataset_ndcld_split_l'))\n",
        "    copytree('/content/Dataset_casia_l/data', os.path.join('/content/drive/My Drive/Toar2/Thesis/Iris_recog/' 'Dataset_casia_split_l'))\n",
        "    copytree('/content/Dataset_berc_l/data', os.path.join('/content/drive/My Drive/Toar2/Thesis/Iris_recog/' 'Dataset_berc_split_l'))\n",
        "    copytree('/content/CombinedDataset_l/data', os.path.join('/content/drive/My Drive/Toar2/Thesis/Iris_recog/' 'CombinedDataset_split_l'))\n",
        "    print('done')\n",
        "    \n",
        "    # some useful commands:\n",
        "# !rm -rf 'data'\n",
        "# !rm -rf '/content/Dataset_berc/data'\n",
        "# !rm -rf '/content/drive/My Drive/Toar2/Thesis/Dataset_ndcld_split'\n",
        "#count files:\n",
        "    # !ls CombinedDataset | wc -l\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ble_Iy0nhChJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#prepare_crop_data used for training data using high res and low res dirs\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import h5py\n",
        "import numpy\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "Patch_size = 32\n",
        "label_size = 20\n",
        "conv_side = 6\n",
        "scale = 4\n",
        "\n",
        "# BORDER_CUT = 8\n",
        "BLOCK_STEP = 16\n",
        "BLOCK_SIZE = 32\n",
        "\n",
        "\n",
        "def prepare_crop_data_new(_path_L, _path_H):\n",
        "    names = [f for f in os.listdir(_path_H) if f.endswith(\".jpg\")]\n",
        "    names = sorted(names)\n",
        "    nums = names.__len__()\n",
        "\n",
        "    data = []\n",
        "    label = []\n",
        "\n",
        "    for i in range(nums):\n",
        "        name = _path_H + names[i]\n",
        "        name_L = _path_L + names[i]\n",
        "\n",
        "        # print('name' ,name)\n",
        "        hr_img = cv2.imread(name, cv2.IMREAD_COLOR)\n",
        "        hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2YCrCb)\n",
        "        hr_img = hr_img[:, :, 0]\n",
        "        shape_H = hr_img.shape\n",
        "\n",
        "        lr_img = cv2.imread(name_L, cv2.IMREAD_COLOR)\n",
        "        lr_img = cv2.cvtColor(lr_img, cv2.COLOR_BGR2YCrCb)\n",
        "        lr_img = lr_img[:, :, 0]\n",
        "        lr_img = cv2.resize(lr_img, (int(shape_H[1] ),int( shape_H[0])))\n",
        "\n",
        "\n",
        "        width_num = (shape_H[0] - (BLOCK_SIZE - BLOCK_STEP) * 2) // BLOCK_STEP\n",
        "        height_num = (shape_H[1] - (BLOCK_SIZE - BLOCK_STEP) * 2) // BLOCK_STEP\n",
        "        for k in range(width_num):\n",
        "            for j in range(height_num):\n",
        "                x = k * BLOCK_STEP\n",
        "                y = j * BLOCK_STEP\n",
        "                hr_patch = hr_img[x: x + BLOCK_SIZE, y: y + BLOCK_SIZE]\n",
        "                lr_patch = lr_img[x: x + BLOCK_SIZE, y: y + BLOCK_SIZE]\n",
        "#                 print('patch coord ', x, x + BLOCK_SIZE,y, y + BLOCK_SIZE)\n",
        "                lr_patch = lr_patch.astype(float) / 255.\n",
        "                hr_patch = hr_patch.astype(float) / 255.\n",
        "\n",
        "                lr = numpy.zeros((1, Patch_size, Patch_size), dtype=numpy.double)\n",
        "                hr = numpy.zeros((1, label_size, label_size), dtype=numpy.double)\n",
        "\n",
        "                lr[0, :, :] = lr_patch\n",
        "                hr[0, :, :] = hr_patch[conv_side: -conv_side, conv_side: -conv_side]\n",
        "\n",
        "                data.append(lr)\n",
        "                label.append(hr)\n",
        "\n",
        "    data = numpy.array(data, dtype=float)\n",
        "    label = numpy.array(label, dtype=float)\n",
        "    return data, label\n",
        "\n",
        "print('done')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEwRnTMnxgCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_hdf5(data, labels, output_filename):\n",
        "    \n",
        "    x = data.astype(numpy.float32)\n",
        "    y = labels.astype(numpy.float32)\n",
        "\n",
        "    with h5py.File(output_filename, 'w') as h:\n",
        "        h.create_dataset('data', data=x, shape=x.shape)\n",
        "        h.create_dataset('label', data=y, shape=y.shape)\n",
        "        # h.create_dataset()\n",
        "\n",
        "def read_training_data(file):\n",
        "    with h5py.File(file, 'r') as hf:\n",
        "        data = numpy.array(hf.get('data'))\n",
        "        label = numpy.array(hf.get('label'))\n",
        "        train_data = numpy.transpose(data, (0, 2, 3, 1))\n",
        "        train_label = numpy.transpose(label, (0, 2, 3, 1))\n",
        "        return train_data, train_label\n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnrgfqejsRnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test\n",
        "if (False):\n",
        "    DATA_PATH = \"Dataset/\"\n",
        "    names = [f for f in os.listdir(DATA_PATH) if f.endswith(\".jpg\")]\n",
        "    names = sorted(names)\n",
        "    nums = names.__len__()\n",
        "\n",
        "    name = DATA_PATH + names[-1]\n",
        "    hr_img = cv2.imread(name, cv2.IMREAD_COLOR)\n",
        "    hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2YCrCb)\n",
        "    hr_img = hr_img[:, :, 0]\n",
        "    shape = hr_img.shape\n",
        "\n",
        "    print(shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnXCYZZjq-l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Input, BatchNormalization, Dropout\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import SGD, Adam\n",
        "import numpy\n",
        "import math\n",
        "\n",
        "def model():\n",
        "    # lrelu = LeakyReLU(alpha=0.1)\n",
        "    SRCNN = Sequential()\n",
        "    SRCNN.add(Conv2D(nb_filter=128, nb_row=9, nb_col=9, init='glorot_uniform',\n",
        "                     activation='relu', border_mode='valid', bias=True, input_shape=(32, 32, 1)))\n",
        "    SRCNN.add(BatchNormalization())\n",
        "    SRCNN.add(Conv2D(nb_filter=64, nb_row=3, nb_col=3, init='glorot_uniform',\n",
        "                     activation='relu', border_mode='same', bias=True))\n",
        "    SRCNN.add(BatchNormalization())\n",
        "\n",
        "    SRCNN.add(Conv2D(nb_filter=1, nb_row=5, nb_col=5, init='glorot_uniform',\n",
        "                     activation='linear', border_mode='valid', bias=True))\n",
        "    SRCNN.add(BatchNormalization())\n",
        "    adam = Adam(lr=0.0003)\n",
        "    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return SRCNN\n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KRlHAfP6OvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "def predict_model():\n",
        "    # lrelu = LeakyReLU(alpha=0.1)\n",
        "    SRCNN = Sequential()\n",
        "    SRCNN.add(Conv2D(nb_filter=128, nb_row=9, nb_col=9, init='glorot_uniform',\n",
        "                     activation='relu', border_mode='valid', bias=True, input_shape=(None, None, 1)))\n",
        "    SRCNN.add(BatchNormalization())\n",
        "    SRCNN.add(Conv2D(nb_filter=64, nb_row=3, nb_col=3, init='glorot_uniform',\n",
        "                     activation='relu', border_mode='same', bias=True))\n",
        "    SRCNN.add(BatchNormalization())\n",
        "    SRCNN.add(Conv2D(nb_filter=1, nb_row=5, nb_col=5, init='glorot_uniform',\n",
        "                     activation='linear', border_mode='valid', bias=True))\n",
        "    SRCNN.add(BatchNormalization())\n",
        "    adam = Adam(lr=0.0003)\n",
        "    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return SRCNN\n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udqH4RMO6OFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train(_dataset_path_l,_dataset_path_h,_checkp_name,_numEpochs,load_h5_data,_train_h5_file,_val_h5_file):\n",
        "    srcnn_model = model()\n",
        "    print(srcnn_model.summary())\n",
        "    if (load_h5_data == True):\n",
        "        data, label = read_training_data(_train_h5_file)\n",
        "        val_data, val_label = read_training_data(_val_h5_file)\n",
        "        print(\"loaded h5 dataset: \" + _train_h5_file)\n",
        "        train_data_t = data\n",
        "        train_label_t = label\n",
        "        val_data_t = val_data\n",
        "        val_label_t = val_label\n",
        "    else:\n",
        "        _train_path_l = _dataset_path_l +\"data/train/\"\n",
        "        _train_path_h = _dataset_path_h +\"data/train/\"\n",
        "        _validation_path_l = _dataset_path_l +\"data/validation/\"\n",
        "        _validation_path_h  = _dataset_path_l +\"data/validation/\"\n",
        "        print(\"using directory dataset\" + _dataset_path)\n",
        "\n",
        "        data, label = prepare_crop_data_new(_train_path_l,_train_path_h)\n",
        "        val_data, val_label = prepare_crop_data_new(_validation_path_l,_validation_path_h)\n",
        "        #transpose to match input shape\n",
        "        train_data_t = numpy.transpose(data, (0, 2, 3, 1))\n",
        "        train_label_t = numpy.transpose(label, (0, 2, 3, 1))\n",
        "        val_data_t = numpy.transpose(val_data, (0, 2, 3, 1))\n",
        "        val_label_t = numpy.transpose(val_label, (0, 2, 3, 1))\n",
        "\n",
        "    # Split the data\n",
        "#     train_data_t, val_data_t, train_label_t, val_label_t = train_test_split(train_data_t, train_label_t, test_size=0.33, shuffle= True)\n",
        "\n",
        "    checkpoint = ModelCheckpoint(_checkp_name, monitor='val_loss', verbose=1, save_best_only=True,\n",
        "                                 save_weights_only=False, mode='min')\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    history =  srcnn_model.fit(train_data_t, train_label_t, batch_size=512, validation_data=(val_data_t, val_label_t),\n",
        "                    callbacks=callbacks_list, shuffle=True, epochs=_numEpochs, verbose=0)\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "    # srcnn_model.load_weights(\"m_model_adam.h5\")\n",
        "\n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXELyzzIDHez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from skimage.measure import compare_ssim as ssim\n",
        "import numpy as np\n",
        "\n",
        "def psnr(target, ref):     \n",
        "    return cv2.PSNR(ref,target)\n",
        "\n",
        "# define function for mean squared error (MSE)\n",
        "def mse(target, ref):\n",
        "    # the MSE between the two images is the sum of the squared difference between the two images\n",
        "    err = np.sum((target.astype('float') - ref.astype('float')) ** 2)\n",
        "    err /= float(target.shape[0] * target.shape[1])\n",
        "    \n",
        "    return err\n",
        "\n",
        "# define function that combines all three image quality metrics\n",
        "def compare_images(target, ref):\n",
        "    scores = []\n",
        "    scores.append(psnr(target, ref))\n",
        "    scores.append(mse(target, ref))\n",
        "    scores.append(ssim(target, ref, multichannel =True))\n",
        "    \n",
        "    return scores\n",
        "\n",
        "\n",
        "def predict(_imgName,_checkp_FileName):\n",
        "    srcnn_model = predict_model()\n",
        "#     srcnn_model.load_weights(\"3051crop_weight_200.h5\")\n",
        "    srcnn_model.load_weights(_checkp_FileName)\n",
        "#     IMG_NAME = \"/content/srcnn_keras_dataset/test/baby_GT.bmp\"\n",
        "  \n",
        "    INPUT_SAVE_NAME = \"input2.jpg\"\n",
        "    OUTPUT_SAVE_NAME = \"pre2.jpg\"\n",
        "\n",
        "\n",
        "    img = cv2.imread(_imgName, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "    shape = img.shape\n",
        "    Y_img = cv2.resize(img[:, :, 0], (shape[1] // scale, shape[0] // scale), cv2.INTER_CUBIC)\n",
        "    Y_img = cv2.resize(Y_img, (shape[1], shape[0]), cv2.INTER_CUBIC)\n",
        "    img[:, :, 0] = Y_img\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n",
        "    cv2.imwrite(INPUT_SAVE_NAME, img)\n",
        "\n",
        "    Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n",
        "    Y[0, :, :, 0] = Y_img.astype(float) / 255.\n",
        "    pre = srcnn_model.predict(Y, batch_size=1) * 255.\n",
        "    pre[pre[:] > 255] = 255\n",
        "    pre[pre[:] < 0] = 0\n",
        "    pre = pre.astype(numpy.uint8)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "    img[6: -6, 6: -6, 0] = pre[0, :, :, 0]\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n",
        "    cv2.imwrite(OUTPUT_SAVE_NAME, img)\n",
        "\n",
        "    # psnr calculation:\n",
        "    im_Orig = cv2.imread(_imgName, cv2.IMREAD_COLOR)\n",
        "    im_Orig = cv2.cvtColor(im_Orig, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "    print (\"im_Orig:\")\n",
        "    cv2_imshow(im_Orig)\n",
        "    im_upscaled = cv2.imread(INPUT_SAVE_NAME, cv2.IMREAD_COLOR)\n",
        "    im_upscaled = cv2.cvtColor(im_upscaled, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "    print (\"im_upscaled bicubic:\")\n",
        "    cv2_imshow(im_upscaled)\n",
        "    im_predicted = cv2.imread(OUTPUT_SAVE_NAME, cv2.IMREAD_COLOR)\n",
        "    im_predicted = cv2.cvtColor(im_predicted, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "    print (\"SR:\")\n",
        "    cv2_imshow(im_predicted)\n",
        "    \n",
        "    print (\"bicubic: (psnr,mse,ssim)\")\n",
        "    print (compare_images(im_Orig, im_upscaled))\n",
        "    print (\"SRCNN : (psnr,mse,ssim)\")\n",
        "    print (compare_images(im_Orig, im_predicted))\n",
        "    print (\"Orig Vs Orig: (psnr,mse,ssim)\")\n",
        "    print (compare_images(im_Orig, im_Orig))\n",
        "    \n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQVJ5qnxiWGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare h5 data files:\n",
        "if (False):\n",
        "    _train_path_H = '/content/Dataset_berc_h/data/train/'\n",
        "    _train_path_L = '/content/Dataset_berc_l/data/train/'\n",
        "    data, label = prepare_crop_data_new(_train_path_L,_train_path_H)\n",
        "    write_hdf5(data, label, \"crop_train_Dataset_berc.h5\")\n",
        "\n",
        "\n",
        "    _validation_path_H = '/content/Dataset_berc_h/data/validation/'\n",
        "    _validation_path_L = '/content/Dataset_berc_l/data/validation/'\n",
        "    data, label = prepare_crop_data_new(_validation_path_L,_validation_path_H)\n",
        "    write_hdf5(data, label, \"crop_val_Dataset_berc.h5\")\n",
        "    print('done berc ')\n",
        "\n",
        "    _train_path_H = '/content/Dataset_casia_h/data/train/'\n",
        "    _train_path_L = '/content/Dataset_casia_l/data/train/'\n",
        "    data, label = prepare_crop_data_new(_train_path_L,_train_path_H)\n",
        "    write_hdf5(data, label, \"crop_train_Dataset_casia.h5\")\n",
        "\n",
        "    _validation_path_H = '/content/Dataset_casia_h/data/validation/'\n",
        "    _validation_path_L = '/content/Dataset_casia_l/data/validation/'\n",
        "    data, label = prepare_crop_data_new(_validation_path_L,_validation_path_H)\n",
        "    write_hdf5(data, label, \"crop_val_Dataset_casia.h5\")\n",
        "    print('done casia')\n",
        "\n",
        "    _train_path_H = '/content/Dataset_ndcld_h/data/train/'\n",
        "    _train_path_L = '/content/Dataset_ndcld_l/data/train/'\n",
        "    data, label = prepare_crop_data_new(_train_path_L,_train_path_H)\n",
        "    write_hdf5(data, label, \"crop_train_Dataset_ndcld.h5\")\n",
        "\n",
        "\n",
        "    _validation_path_H = '/content/Dataset_ndcld_h/data/validation/'\n",
        "    _validation_path_L = '/content/Dataset_ndcld_l/data/validation/'\n",
        "    data, label = prepare_crop_data_new(_validation_path_L,_validation_path_H)\n",
        "    write_hdf5(data, label, \"crop_val_Dataset_ndcld.h5\")\n",
        "    print('done ndcld')\n",
        "\n",
        "    _train_path_H = '/content/CombinedDataset_h/data/train/'\n",
        "    _train_path_L = '/content/CombinedDataset_l/data/train/'\n",
        "    data, label = prepare_crop_data_new(_train_path_L,_train_path_H)\n",
        "    write_hdf5(data, label, \"crop_train_CombinedDataset.h5\")\n",
        "\n",
        "\n",
        "    _validation_path_H = '/content/CombinedDataset_h/data/validation/'\n",
        "    _validation_path_L = '/content/CombinedDataset_l/data/validation/'\n",
        "    data, label = prepare_crop_data_new(_validation_path_L,_validation_path_H)\n",
        "    write_hdf5(data, label, \"crop_val_CombinedDataset.h5\")\n",
        "    print('done combined')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccE2aFQINrx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy weights and datasets from drive to local env:\n",
        "!cp -avr '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_datasets_h5/' \"/content/\" \n",
        "print('done ')\n",
        "!cp -avr '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_w/' \"/content/\"\n",
        "print('done ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sREjXiUko9-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare h5 dataset files (maybe saves memory )\n",
        "# Train:\n",
        "\n",
        "CHEKPOINT_NAME =\"SRCNN_check_Dataset_berc_30Ep.h5\"\n",
        "NUM_EPOCHS = 30;\n",
        "H5_DATASET_TRAIN = '/content/crop_train_Dataset_berc.h5'\n",
        "H5_DATASET_VAL = '/content/crop_val_Dataset_berc.h5'\n",
        "load_h5_weights_Toggle = True\n",
        "train(H5_DATASET_TRAIN,H5_DATASET_VAL,CHEKPOINT_NAME,NUM_EPOCHS,load_h5_weights_Toggle,H5_DATASET_TRAIN,H5_DATASET_VAL)\n",
        "!cp -avr \"SRCNN_check_Dataset_berc_30Ep.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_w/'\n",
        "print('done berc')\n",
        "#\n",
        "DATA_PATH = \"Dataset_casia/\"\n",
        "CHEKPOINT_NAME =\"SRCNN_check_Dataset_casia_30Ep.h5\"\n",
        "NUM_EPOCHS = 30;\n",
        "H5_DATASET_TRAIN = '/content/crop_train_Dataset_casia.h5'\n",
        "H5_DATASET_VAL = '/content/crop_val_Dataset_casia.h5'\n",
        "train(H5_DATASET_TRAIN,H5_DATASET_VAL,CHEKPOINT_NAME,NUM_EPOCHS,load_h5_weights_Toggle,H5_DATASET_TRAIN,H5_DATASET_VAL)\n",
        "!cp -avr \"SRCNN_check_Dataset_casia_30Ep.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_w/'\n",
        "print('done casia')\n",
        "#\n",
        "DATA_PATH = \"Dataset_ndcld/\"\n",
        "CHEKPOINT_NAME =\"SRCNN_check_Dataset_ndcld_30Ep.h5\"\n",
        "NUM_EPOCHS = 30;\n",
        "H5_DATASET_TRAIN = '/content/crop_train_Dataset_ndcld.h5'\n",
        "H5_DATASET_VAL = '/content/crop_val_Dataset_ndcld.h5'\n",
        "train(H5_DATASET_TRAIN,H5_DATASET_VAL,CHEKPOINT_NAME,NUM_EPOCHS,load_h5_weights_Toggle,H5_DATASET_TRAIN,H5_DATASET_VAL)\n",
        "!cp -avr \"SRCNN_check_Dataset_ndcld_30Ep.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_w/'\n",
        "print('done ndcld')\n",
        "#\n",
        "DATA_PATH = \"CombinedDataset/\"\n",
        "CHEKPOINT_NAME =\"SRCNN_check_CombinedDataset_35Ep.h5\"\n",
        "NUM_EPOCHS = 35;\n",
        "H5_DATASET_TRAIN = '/content/crop_train_CombinedDataset.h5'\n",
        "H5_DATASET_VAL = '/content/crop_val_CombinedDataset.h5'\n",
        "train(H5_DATASET_TRAIN,H5_DATASET_VAL,CHEKPOINT_NAME,NUM_EPOCHS,load_h5_weights_Toggle,H5_DATASET_TRAIN,H5_DATASET_VAL)\n",
        "!cp -avr \"SRCNN_check_CombinedDataset_35Ep.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_w/'\n",
        "print('done CombinedDataset')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9kCxSOCC9KC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy saved w to drive\n",
        "\n",
        "print('saved_weights copied')\n",
        "if (False):\n",
        "    # copy saved datasets to drive\n",
        "    !cp -avr \"/content/crop_train_Dataset_berc.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_datasets_h5/'\n",
        "    !cp -avr \"/content/crop_val_Dataset_berc.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_datasets_h5/'\n",
        "    !cp -avr \"/content/crop_train_Dataset_casia.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_datasets_h5/'\n",
        "    !cp -avr \"/content/crop_val_Dataset_casia.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_datasets_h5/'\n",
        "    !cp -avr \"/content/crop_train_Dataset_ndcld.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_datasets_h5/'\n",
        "    !cp -avr \"/content/crop_val_Dataset_ndcld.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_datasets_h5/'\n",
        "    !cp -avr \"/content/crop_train_CombinedDataset.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_datasets_h5/'\n",
        "    !cp -avr \"/content/crop_val_CombinedDataset.h5\" '/content/drive/My Drive/Toar2/Thesis/Iris_recog/saved_datasets_h5/'\n",
        "\n",
        "    print('saved_weights copied')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8EcF85zpFZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!cp -avr '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Dataset_ndcld_split_h/validation/testing-live-05812d905.jpg'  '/content/1.jpg'\n",
        "IMG_NAME_H = \"1.jpg\"\n",
        "!cp -avr '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Dataset_ndcld_split_l/validation/testing-live-05812d905.jpg'  '/content/2.jpg'\n",
        "IMG_NAME_L = \"2.jpg\"\n",
        "# IMG_NAME = \"/content/Validation_old/live-850-068_000_i_to_00_00_000_01_00_l_20050928131705.jpg\"\n",
        "# IMG_NAME = \"/content/Dataset_ndcld/data/validation/testing-live-05871d852.jpg\"\n",
        "\n",
        "# CHEKPOINT_NAME = '/content/SRCNN_check_casia_50Ep_L0_00033.h5'\n",
        "# CHEKPOINT_NAME = '/content/SRCNN_check_ber_50Ep_L0_00089.h5'\n",
        "\n",
        "CHEKPOINT_NAME =\"/content/saved_w/SRCNN_check_Dataset_ndcld_30Ep.h5\"\n",
        "\n",
        "# predict:\n",
        "predict(IMG_NAME_H,CHEKPOINT_NAME)\n",
        "\n",
        "predict_clean(IMG_NAME_L,IMG_NAME_H,CHEKPOINT_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg_3FPbak6gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TOFIX\n",
        "import os \n",
        "\n",
        "def predict_clean(_imgNameLow,_imgNameHigh,_checkp_FileName):\n",
        "    srcnn_model = predict_model()\n",
        "#   srcnn_model.load_weights(\"3051crop_weight_200.h5\")\n",
        "    srcnn_model.load_weights(_checkp_FileName)\n",
        "#   IMG_NAME = \"/content/srcnn_keras_dataset/test/baby_GT.bmp\"\n",
        "    OUT_PATH = '/content/SR_Result_x4'\n",
        "    INPUT_SAVE_NAME = \"input2.jpg\"\n",
        "    OUTPUT_SAVE_NAME = \"pre2.jpg\"\n",
        "\n",
        "    img = cv2.imread(_imgNameLow, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "    shape = img.shape\n",
        "    Y_img = cv2.resize(img[:, :, 0], (shape[1] * scale, shape[0] * scale), cv2.INTER_CUBIC)\n",
        "    img = cv2.resize(img, (shape[1] * scale, shape[0] * scale), cv2.INTER_CUBIC)\n",
        "    img[:, :, 0] = Y_img\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n",
        "    cv2.imwrite(INPUT_SAVE_NAME, img)\n",
        "    cv2_imshow(img)\n",
        "    \n",
        "    Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n",
        "    Y[0, :, :, 0] = Y_img.astype(float) / 255.\n",
        "    pre = srcnn_model.predict(Y, batch_size=1) * 255.\n",
        "    pre[pre[:] > 255] = 255\n",
        "    pre[pre[:] < 0] = 0\n",
        "    pre = pre.astype(numpy.uint8)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "    img[6: -6, 6: -6, 0] = pre[0, :, :, 0]\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n",
        "    cv2.imwrite(OUTPUT_SAVE_NAME, img)\n",
        "\n",
        "    # psnr calculation:\n",
        "    im_Orig = cv2.imread(_imgNameHigh, cv2.IMREAD_COLOR)\n",
        "    im_Orig = cv2.cvtColor(im_Orig, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "    print (\"im_Orig:\")\n",
        "    cv2_imshow(im_Orig)\n",
        "    im_upscaled = cv2.imread(INPUT_SAVE_NAME, cv2.IMREAD_COLOR)\n",
        "    im_upscaled = cv2.cvtColor(im_upscaled, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "    print (\"im_upscaled bicubic:\")\n",
        "    cv2_imshow(im_upscaled)\n",
        "    im_predicted = cv2.imread(OUTPUT_SAVE_NAME, cv2.IMREAD_COLOR)\n",
        "    im_predicted = cv2.cvtColor(im_predicted, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "    print (\"SR:\")\n",
        "    cv2_imshow(im_predicted)\n",
        "    \n",
        "    print (\"bicubic: (psnr,mse,ssim)\")\n",
        "    print (compare_images(im_Orig, im_upscaled))\n",
        "    print (\"SRCNN : (psnr,mse,ssim)\")\n",
        "    print (compare_images(im_Orig, im_predicted))\n",
        "    print (\"Orig Vs Orig: (psnr,mse,ssim)\")\n",
        "    print (compare_images(im_Orig, im_Orig))\n",
        "\n",
        "    # # psnr calculation:\n",
        "    # im_Orig = cv2.imread(_imgNameHigh, cv2.IMREAD_COLOR)\n",
        "    # im_Orig = cv2.cvtColor(im_Orig, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "    # print (\"im_Orig:\")\n",
        "    # im_upscaled = cv2.imread(INPUT_SAVE_NAME, cv2.IMREAD_COLOR)\n",
        "    # im_upscaled = cv2.cvtColor(im_upscaled, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "    # print (\"im_upscaled bicubic:\")\n",
        "    # im_predicted = cv2.imread(OUTPUT_SAVE_NAME, cv2.IMREAD_COLOR)\n",
        "    # im_predicted = cv2.cvtColor(im_predicted, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "    # print (\"SR:\")\n",
        "\n",
        "    \n",
        "    # print (\"bicubic: (psnr,mse,ssim)\")\n",
        "    # print (compare_images(im_Orig, im_upscaled))\n",
        "    # print (\"SRCNN : (psnr,mse,ssim)\")\n",
        "    # print (compare_images(im_Orig, im_predicted))\n",
        "    \n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akRKLzbpHNyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TOFIX\n",
        "import os \n",
        "import time\n",
        "\n",
        "def generate_sr_images(_imgNameLowDir,_checkp_FileName):\n",
        "    srcnn_model = predict_model()\n",
        "    srcnn_model.load_weights(_checkp_FileName)\n",
        "\n",
        "    if not os.path.exists(_imgNameLowDir):\n",
        "        raise OSError('_imgNameLowDir does not exist')    \n",
        "    out_dir = _imgNameLowDir + '/SR'\n",
        "    if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "\n",
        "    names = [f for f in os.listdir(_imgNameLowDir) if f.endswith(\".jpg\")]\n",
        "    names = sorted(names)\n",
        "    nums = names.__len__()\n",
        "    counter = 0\n",
        "\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    textfile = open('SR_GEN_LOG' + timestr +'.txt', 'w') \n",
        "  \n",
        "    for i in range(nums):\n",
        "        name =os.path.join(_imgNameLowDir,names[i])\n",
        "        # print('name' ,name)\n",
        "        img = cv2.imread(name, cv2.IMREAD_COLOR)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "        shape = img.shape\n",
        "        Y_img = cv2.resize(img[:, :, 0], (shape[1] * scale, shape[0] * scale), cv2.INTER_CUBIC)\n",
        "        img = cv2.resize(img, (shape[1] * scale, shape[0] * scale), cv2.INTER_CUBIC)\n",
        "        img[:, :, 0] = Y_img\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n",
        "        #\n",
        "        Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n",
        "        Y[0, :, :, 0] = Y_img.astype(float) / 255.\n",
        "        pre = srcnn_model.predict(Y, batch_size=1) * 255.\n",
        "        pre[pre[:] > 255] = 255\n",
        "        pre[pre[:] < 0] = 0\n",
        "        pre = pre.astype(numpy.uint8)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "        img[6: -6, 6: -6, 0] = pre[0, :, :, 0]\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n",
        "        cv2.imwrite(os.path.join(out_dir,names[i]), img)\n",
        "\n",
        "        counter  += 1   \n",
        "        print(str(counter) + '# file:' + names[i] + '\\n', file = textfile) \n",
        "        #TBD add validation print\n",
        "    # print('Copied ' + str(counter) + out_dir)\n",
        "    textfile.close() \n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQxgeGDHS1BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf 'low_res_ndcld'\n",
        "!rm -rf 'low_res_casia'\n",
        "!rm -rf 'low_res_berc'\n",
        "!rm -rf 'low_res_combined'\n",
        "#copy from drive split dataset , validation:\n",
        "# low res val\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_ndcld_split_l/validation/' low_res_ndcld\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_casia_split_l/validation/' low_res_casia\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_berc_split_l/validation/' low_res_berc\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/CombinedDataset_split_l/validation/' low_res_combined\n",
        "print('done')\n",
        "# high res val\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_ndcld_split_h/validation/' high_res_ndcld\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_casia_split_h/validation/' high_res_casia\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_berc_split_h/validation/' high_res_berc\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/CombinedDataset_split_h/validation/' high_res_combined\n",
        "#copy from drive split dataset , training:\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_ndcld_split_l/train/' low_res_ndcld_train\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_casia_split_l/train/' low_res_casia_train\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_berc_split_l/train/' low_res_berc_train\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/CombinedDataset_split_l/train/' low_res_combined_train\n",
        "print('train done')\n",
        "# high res val\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_ndcld_split_h/train/' high_res_ndcld_train\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_casia_split_h/train/' high_res_casia_train\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/Dataset_berc_split_h/train/' high_res_berc_train\n",
        "!cp -ar '/content/drive/My Drive/Toar2/Thesis/Iris_recog/Iris_Dataset/CombinedDataset_split_h/train/' high_res_combined_train\n",
        "print('train done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amr--tjHSxTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gen SR for validation\n",
        "CHEKPOINT_NAME =\"/content/saved_w/SRCNN_check_Dataset_casia_30Ep.h5\"\n",
        "LR_IMG_DIR = '/content/low_res_casia'\n",
        "generate_sr_images(LR_IMG_DIR,CHEKPOINT_NAME)\n",
        "print('done casia')\n",
        "CHEKPOINT_NAME =\"/content/saved_w/SRCNN_check_Dataset_berc_30Ep.h5\"\n",
        "LR_IMG_DIR = '/content/low_res_berc'\n",
        "generate_sr_images(LR_IMG_DIR,CHEKPOINT_NAME)\n",
        "print('done berc')\n",
        "CHEKPOINT_NAME =\"/content/saved_w/SRCNN_check_Dataset_ndcld_30Ep.h5\"\n",
        "LR_IMG_DIR = '/content/low_res_ndcld'\n",
        "generate_sr_images(LR_IMG_DIR,CHEKPOINT_NAME)\n",
        "print('done ndckd')\n",
        "CHEKPOINT_NAME =\"/content/saved_w/SRCNN_check_CombinedDataset_35Ep.h5\"\n",
        "LR_IMG_DIR = '/content/low_res_combined'\n",
        "generate_sr_images(LR_IMG_DIR,CHEKPOINT_NAME)\n",
        "print('done combined')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igSf2p8pUrEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gen SR for train data\n",
        "CHEKPOINT_NAME =\"/content/saved_w/SRCNN_check_Dataset_casia_30Ep.h5\"\n",
        "LR_IMG_DIR = '/content/low_res_casia_train'\n",
        "generate_sr_images(LR_IMG_DIR,CHEKPOINT_NAME)\n",
        "print('done casia')\n",
        "CHEKPOINT_NAME =\"/content/saved_w/SRCNN_check_Dataset_berc_30Ep.h5\"\n",
        "LR_IMG_DIR = '/content/low_res_berc_train'\n",
        "generate_sr_images(LR_IMG_DIR,CHEKPOINT_NAME)\n",
        "print('done berc')\n",
        "CHEKPOINT_NAME =\"/content/saved_w/SRCNN_check_Dataset_ndcld_30Ep.h5\"\n",
        "LR_IMG_DIR = '/content/low_res_ndcld_train'\n",
        "generate_sr_images(LR_IMG_DIR,CHEKPOINT_NAME)\n",
        "print('done ndckd')\n",
        "CHEKPOINT_NAME =\"/content/saved_w/SRCNN_check_CombinedDataset_35Ep.h5\"\n",
        "LR_IMG_DIR = '/content/low_res_combined_train'\n",
        "generate_sr_images(LR_IMG_DIR,CHEKPOINT_NAME)\n",
        "print('done combined')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga81Y9fnhGAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy validation (test) SR images to drive\n",
        "!cp -avr '/content/low_res_casia/SR' '/content/drive/My Drive/Toar2/Thesis/Iris_recog/SR_Scaled_test_images/casia_test_sr/'\n",
        "print('done')\n",
        "!cp -avr '/content/low_res_berc/SR' '/content/drive/My Drive/Toar2/Thesis/Iris_recog/SR_Scaled_test_images/berc_test_sr/'\n",
        "print('done')\n",
        "!cp -ar '/content/low_res_ndcld/SR' '/content/drive/My Drive/Toar2/Thesis/Iris_recog/SR_Scaled_test_images/ndcld_test_sr/'\n",
        "print('done')\n",
        "!cp -ar '/content/low_res_combined/SR' '/content/drive/My Drive/Toar2/Thesis/Iris_recog/SR_Scaled_test_images/combined_test_sr/'\n",
        "print('done')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYibheIvWEmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy train () SR images to drive\n",
        "# !mkdir '/content/drive/My Drive/Toar2/Thesis/Iris_recog/SR_Scaled_train_images/combined_train_sr/'\n",
        "\n",
        "!cp -avr '/content/low_res_casia_train/SR' '/content/drive/My Drive/Toar2/Thesis/Iris_recog/SR_Scaled_train_images/casia_train_sr/'\n",
        "print('done train')\n",
        "!cp -avr '/content/low_res_berc_train/SR' '/content/drive/My Drive/Toar2/Thesis/Iris_recog/SR_Scaled_train_images/berc_train_sr/'\n",
        "print('done train')\n",
        "!cp -ar '/content/low_res_ndcld_train/SR' '/content/drive/My Drive/Toar2/Thesis/Iris_recog/SR_Scaled_train_images/ndcld_train_sr/'\n",
        "print('done train')\n",
        "!cp -ar '/content/low_res_combined_train/SR' '/content/drive/My Drive/Toar2/Thesis/Iris_recog/SR_Scaled_train_images/combined_train_sr/'\n",
        "print('done train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZC-XZNAWmSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sze-azhUJte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def print_error_statistics(_imgNameLow_Dir,_imgNameHigh_Dir,_imgNameSR_Dir,dataset_name):\n",
        "    \n",
        "    names = [f for f in os.listdir(_imgNameLow_Dir) if f.endswith(\".jpg\")]\n",
        "    names = sorted(names)\n",
        "    nums = names.__len__()\n",
        "    counter = 0\n",
        "\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    textfile = open('RESULT_SR_VS_ORIG_STATISTICS_' + dataset_name + '_' + timestr +'.txt', 'w') \n",
        "    print('----statistics for:  +' + dataset_name+ 'dataset' + '\\n\\n', file = textfile) \n",
        "    for i in range(nums):\n",
        "        name_low =os.path.join(_imgNameLow_Dir,names[i])\n",
        "        name_high =os.path.join(_imgNameHigh_Dir,names[i])\n",
        "        name_SR =os.path.join(_imgNameSR_Dir,names[i])\n",
        "        img_l = cv2.imread(name_low, cv2.IMREAD_COLOR)\n",
        "        img_h = cv2.imread(name_high, cv2.IMREAD_COLOR)\n",
        "        img_sr = cv2.imread(name_SR, cv2.IMREAD_COLOR)\n",
        "        shape = img_l.shape\n",
        "        im_l_upscaled = cv2.cvtColor(img_l, cv2.COLOR_BGR2YCrCb)\n",
        "        im_l_upscaled = cv2.resize(im_l_upscaled[:, :, 0], (shape[1] * scale, shape[0] * scale), cv2.INTER_CUBIC)[6: -6, 6: -6]\n",
        "\n",
        "        img_h = cv2.cvtColor(img_h, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "        img_sr = cv2.cvtColor(img_sr, cv2.COLOR_BGR2YCrCb)[6: -6, 6: -6, 0]\n",
        "       \n",
        "        if (i == 1):\n",
        "            print (\"im_l_upscaled:\")\n",
        "            cv2_imshow(im_l_upscaled)\n",
        "            print (\"img_h:\")\n",
        "            cv2_imshow(img_h)\n",
        "            print (\"img_sr:\")\n",
        "            cv2_imshow(img_sr)\n",
        "        print(\"filename: \" + str( names[i] ) + '\\n', file = textfile)\n",
        "        print(\"Orig Vs biCubic(CV2) upscaled: (psnr,mse,ssim)\" + str( compare_images(img_h, im_l_upscaled)) + '\\n', file = textfile) \n",
        "        print(\"orig Vs SR: (psnr,mse,ssim)\" + str( compare_images(img_h, img_sr)  ) + '\\n', file = textfile) \n",
        "        print(\"Orig Vs Orig: (psnr,mse,ssim)\" + str( compare_images(img_h, img_h)   ) + '\\n', file = textfile)\n",
        "        print(\"-------------------------------------\" + '\\n', file = textfile) \n",
        "        \n",
        "    textfile.close() \n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kQxTnCOiHVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run statistis on all *validation* datasets and save log file\n",
        "_imgNameLow_Dir = '/content/low_res_casia/'\n",
        "_imgNameHigh_Dir = '/content/high_res_casia/'\n",
        "_imgNameSR_Dir  = '/content/low_res_casia/SR/'\n",
        "print_error_statistics(_imgNameLow_Dir,_imgNameHigh_Dir,_imgNameSR_Dir,\"casia\")\n",
        "print('casia')\n",
        "\n",
        "_imgNameLow_Dir = '/content/low_res_berc/'\n",
        "_imgNameHigh_Dir = '/content/high_res_berc/'\n",
        "_imgNameSR_Dir  = '/content/low_res_berc/SR/'\n",
        "print_error_statistics(_imgNameLow_Dir,_imgNameHigh_Dir,_imgNameSR_Dir,\"berc\")\n",
        "print('berc')\n",
        "\n",
        "_imgNameLow_Dir = '/content/low_res_ndcld/'\n",
        "_imgNameHigh_Dir = '/content/high_res_ndcld/'\n",
        "_imgNameSR_Dir  = '/content/low_res_ndcld/SR/'\n",
        "print_error_statistics(_imgNameLow_Dir,_imgNameHigh_Dir,_imgNameSR_Dir,\"ndcld\")\n",
        "print('ndcld')\n",
        "\n",
        "_imgNameLow_Dir = '/content/low_res_combined/'\n",
        "_imgNameHigh_Dir = '/content/high_res_combined/'\n",
        "_imgNameSR_Dir  = '/content/low_res_combined/SR/'\n",
        "print_error_statistics(_imgNameLow_Dir,_imgNameHigh_Dir,_imgNameSR_Dir,\"combined\")\n",
        "print('combined')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niUjwgqiYg0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run statistis on all *Train* dataset and save log file\n",
        "_imgNameLow_Dir = '/content/low_res_casia_train/'\n",
        "_imgNameHigh_Dir = '/content/high_res_casia_train/'\n",
        "_imgNameSR_Dir  = '/content/low_res_casia_train/SR/'\n",
        "print_error_statistics(_imgNameLow_Dir,_imgNameHigh_Dir,_imgNameSR_Dir,\"casia_train\")\n",
        "print('casia _train')\n",
        "\n",
        "_imgNameLow_Dir = '/content/low_res_berc_train/'\n",
        "_imgNameHigh_Dir = '/content/high_res_berc_train/'\n",
        "_imgNameSR_Dir  = '/content/low_res_berc_train/SR/'\n",
        "print_error_statistics(_imgNameLow_Dir,_imgNameHigh_Dir,_imgNameSR_Dir,\"berc_train\")\n",
        "print('berc _train')\n",
        "\n",
        "_imgNameLow_Dir = '/content/low_res_ndcld_train/'\n",
        "_imgNameHigh_Dir = '/content/high_res_ndcld_train/'\n",
        "_imgNameSR_Dir  = '/content/low_res_ndcld_train/SR/'\n",
        "print_error_statistics(_imgNameLow_Dir,_imgNameHigh_Dir,_imgNameSR_Dir,\"ndcld_train\")\n",
        "print('ndcld _train')\n",
        "\n",
        "_imgNameLow_Dir = '/content/low_res_combined_train/'\n",
        "_imgNameHigh_Dir = '/content/high_res_combined_train/'\n",
        "_imgNameSR_Dir  = '/content/low_res_combined_train/SR/'\n",
        "print_error_statistics(_imgNameLow_Dir,_imgNameHigh_Dir,_imgNameSR_Dir,\"combined_train\")\n",
        "print('combined _train')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}